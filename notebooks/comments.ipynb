{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import swifter\n",
    "import pickle\n",
    "import random\n",
    "import gzip\n",
    "import json\n",
    "import zstd\n",
    "\n",
    "class Zreader:\n",
    "\n",
    "    def __init__(self, file, chunk_size=16384):\n",
    "        '''Init method'''\n",
    "        self.fh = open(file,'rb')\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dctx = zstd.ZstdDecompressor()\n",
    "        self.reader = self.dctx.stream_reader(self.fh)\n",
    "        self.buffer = ''\n",
    "\n",
    "    def readlines(self):\n",
    "        '''Generator method that creates an iterator for each line of JSON'''\n",
    "        while True:\n",
    "            chunk = self.reader.read(self.chunk_size).decode(\"utf-8\", errors=\"ignore\")\n",
    "            if not chunk:\n",
    "                break\n",
    "            lines = (self.buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            for line in lines[:-1]:\n",
    "                yield line\n",
    "\n",
    "            self.buffer = lines[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(\"/dlabdata1/youtube_large/yt_metadata_helper.feather\")\n",
    "df2 = pd.read_csv(\"/dlabdata1/youtube_large/df_channels_en.tsv.gz\", compression=\"infer\", sep=\"\\t\")\n",
    "num_to_display_id = {v: k for k, v in df.display_id.to_dict().items()}\n",
    "num_to_channel_id = {v: k for k, v in df2.channel.to_dict().items()}\n",
    "num_comms = {k: None for k, v in num_to_display_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First pass: read comments\n",
    "reader = Zreader(\"/dlabdata1/youtube_large/youtube_comments.ndjson.zst\", chunk_size=16384)\n",
    "\n",
    "idx = 0\n",
    "\n",
    "# reads each line from the reader\n",
    "for line in reader.readlines():\n",
    "    idx += 1\n",
    "    \n",
    "    if idx % 10000000 == 0:\n",
    "        print(idx)\n",
    "\n",
    "    try:\n",
    "        line = line.split(\",\")\n",
    "        video_id = line[2][1:-1] \n",
    "        author = line[0]\n",
    "    except:\n",
    "        print(\"error parsing line\")\n",
    "        continue\n",
    "        \n",
    "    if video_id not in num_comms:\n",
    "        continue\n",
    "        \n",
    "    if num_comms[video_id] is None:\n",
    "        num_comms[video_id] = 1\n",
    "    else:\n",
    "        num_comms[video_id] += 1\n",
    "        \n",
    "comm_series = pd.Series(num_comms)\n",
    "num_comments = pd.DataFrame(comm_series).reset_index()\n",
    "num_comments.columns = [\"display_id\", \"num_comms\"]\n",
    "num_comments.to_csv(\"/dlabdata1/youtube_large/num_comments.tsv.gz\", compression=\"infer\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second pass: prints comments with for videos with more than 5 comments\n",
    "import gzip\n",
    "reader = Zreader(\"/dlabdata1/youtube_large/youtube_comments.ndjson.zst\", chunk_size=16384)\n",
    "\n",
    "author_orig = \"\"\n",
    "author_count = 0\n",
    "idx = 0\n",
    "with gzip.open(\"/dlabdata1/youtube_large/youtube_comments.tsv.gz\", \"w\") as f:\n",
    "    \n",
    "    f.write((\"\\t\".join([\"author\", \"video_id\", \"likes\", \"replies\"]) + \"\\n\").encode())\n",
    "\n",
    "    # reads each line from the reader\n",
    "    for line in reader.readlines():\n",
    "        idx += 1\n",
    "        \n",
    "        if idx == 1:\n",
    "            continue\n",
    "\n",
    "        if idx % 10000000 == 0:\n",
    "            print(idx)\n",
    "\n",
    "        try:\n",
    "            line = line.split(\",\")\n",
    "            video_id = line[2][1:-1] \n",
    "            author = line[0]\n",
    "\n",
    "            likes = line[5]\n",
    "            replies = line[6]\n",
    "        except:\n",
    "            print(\"error parsing line\")\n",
    "            continue\n",
    "            \n",
    "        if author != author_orig:\n",
    "            author_orig = author\n",
    "            author_count += 1\n",
    "\n",
    "        if video_id in num_comms and num_comms[video_id] > 30:\n",
    "            f.write((\"\\t\".join([str(author_count), video_id, str(likes), str(replies)])+ \"\\n\").encode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Third pass: author per comment helper\n",
    "tmp_authors = []\n",
    "for df in  pd.read_csv(\"/dlabdata1/youtube_large/youtube_comments.tsv.gz\", compression=\"infer\", sep=\"\\t\", \n",
    "                       chunksize=10000000):\n",
    "    print(\"x\")\n",
    "    tmp_authors.append(df.groupby(\"author\").video_id.count())\n",
    "tmp = pd.concat(tmp_authors)\n",
    "tmp = tmp.reset_index()\n",
    "num_comments_author = tmp.groupby(\"author\").video_id.sum()\n",
    "num_comments_author.reset_index().to_csv(\"/dlabdata1/youtube_large/num_comments_authors.tsv.gz\", compression=\"infer\", \n",
    "                           sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
